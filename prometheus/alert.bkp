groups:
  # - name: infrastructure
  #   interval: 30s
  #   rules:
  #     # Container down alert
  #     - alert: ContainerDown
  #       expr: up == 0
  #       for: 1m
  #       labels:
  #         severity: critical
  #       annotations:
  #         summary: "Container {{ $labels.job }} is down"
  #         description: "Container {{ $labels.job }} has been down for more than 1 minute."

  #     # High CPU usage
  #     - alert: HighCPUUsage
  #       expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
  #       for: 5m
  #       labels:
  #         severity: warning
  #       annotations:
  #         summary: "High CPU usage on {{ $labels.instance }}"
  #         description: "CPU usage is above 80% (current: {{ $value | humanize }}%)"

  #     # Critical CPU usage
  #     - alert: CriticalCPUUsage
  #       expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 90
  #       for: 2m
  #       labels:
  #         severity: critical
  #       annotations:
  #         summary: "Critical CPU usage on {{ $labels.instance }}"
  #         description: "CPU usage is above 90% (current: {{ $value | humanize }}%)"

  #     # High memory usage
  #     - alert: HighMemoryUsage
  #       expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 80
  #       for: 5m
  #       labels:
  #         severity: warning
  #       annotations:
  #         summary: "High memory usage on {{ $labels.instance }}"
  #         description: "Memory usage is above 80% (current: {{ $value | humanize }}%)"

  #     # Critical memory usage
  #     - alert: CriticalMemoryUsage
  #       expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
  #       for: 2m
  #       labels:
  #         severity: critical
  #       annotations:
  #         summary: "Critical memory usage on {{ $labels.instance }}"
  #         description: "Memory usage is above 90% (current: {{ $value | humanize }}%)"

  #     # Low disk space
  #     - alert: LowDiskSpace
  #       expr: (node_filesystem_avail_bytes{fstype!~"tmpfs|fuse.lxcfs"} / node_filesystem_size_bytes{fstype!~"tmpfs|fuse.lxcfs"}) * 100 < 20
  #       for: 5m
  #       labels:
  #         severity: warning
  #       annotations:
  #         summary: "Low disk space on {{ $labels.instance }}"
  #         description: "Disk {{ $labels.mountpoint }} has less than 20% free space (current: {{ $value | humanize }}%)"

  #     # Critical disk space
  #     - alert: CriticalDiskSpace
  #       expr: (node_filesystem_avail_bytes{fstype!~"tmpfs|fuse.lxcfs"} / node_filesystem_size_bytes{fstype!~"tmpfs|fuse.lxcfs"}) * 100 < 10
  #       for: 2m
  #       labels:
  #         severity: critical
  #       annotations:
  #         summary: "Critical disk space on {{ $labels.instance }}"
  #         description: "Disk {{ $labels.mountpoint }} has less than 10% free space (current: {{ $value | humanize }}%)"

  # - name: database
  #   interval: 30s
  #   rules:
  #     # PostgreSQL down
  #     - alert: PostgreSQLDown
  #       expr: pg_up == 0
  #       for: 1m
  #       labels:
  #         severity: critical
  #       annotations:
  #         summary: "PostgreSQL is down"
  #         description: "PostgreSQL database has been unreachable for more than 1 minute."

  #     # Too many connections
  #     - alert: PostgreSQLTooManyConnections
  #       expr: sum by (instance) (pg_stat_activity_count) > 80
  #       for: 5m
  #       labels:
  #         severity: warning
  #       annotations:
  #         summary: "PostgreSQL has too many connections"
  #         description: "PostgreSQL has more than 80 connections (current: {{ $value }})"

  #     # High connection usage percentage
  #     - alert: PostgreSQLHighConnectionUsage
  #       expr: (sum by (instance) (pg_stat_activity_count) / pg_settings_max_connections) * 100 > 80
  #       for: 5m
  #       labels:
  #         severity: warning
  #       annotations:
  #         summary: "PostgreSQL connection pool usage high"
  #         description: "PostgreSQL is using more than 80% of max connections (current: {{ $value | humanize }}%)"

  # - name: storage
  #   interval: 30s
  #   rules:
  #     # MinIO down
  #     - alert: MinIODown
  #       expr: up{job="minio"} == 0
  #       for: 1m
  #       labels:
  #         severity: critical
  #       annotations:
  #         summary: "MinIO storage is down"
  #         description: "MinIO has been unreachable for more than 1 minute."

  #     # MinIO high error rate
  #     - alert: MinIOHighErrorRate
  #       expr: rate(minio_s3_requests_errors_total[5m]) > 0.05
  #       for: 5m
  #       labels:
  #         severity: warning
  #       annotations:
  #         summary: "MinIO has high error rate"
  #         description: "MinIO error rate is above 5% (current: {{ $value | humanizePercentage }})"

  - name: workers
    interval: 30s
    rules:
      # Worker down
      - alert: WorkerDown
        expr: up{service="worker"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Worker {{ $labels.worker_type }} is down"
          description: "Worker {{ $labels.worker_type }} has been down for more than 2 minutes."

      # High worker error rate (when metrics are implemented)
      - alert: WorkerHighErrorRate
        expr: rate(worker_processing_errors_total[10m]) / rate(worker_processing_total[10m]) > 0.1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Worker {{ $labels.worker_type }} has high error rate"
          description: "Worker {{ $labels.worker_type }} error rate is above 10% (current: {{ $value | humanizePercentage }})"

      # Worker processing slow (when metrics are implemented)
      - alert: WorkerProcessingSlow
        expr: histogram_quantile(0.95, rate(worker_processing_duration_seconds_bucket[10m])) > 300
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Worker {{ $labels.worker_type }} is processing slowly"
          description: "Worker {{ $labels.worker_type }} 95th percentile processing time is above 5 minutes (current: {{ $value | humanizeDuration }})"

      # Worker queue growing (when metrics are implemented)
      - alert: WorkerQueueGrowing
        expr: worker_queue_depth > 50 and rate(worker_queue_depth[10m]) > 0
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "Worker {{ $labels.worker_type }} queue is growing"
          description: "Worker {{ $labels.worker_type }} has {{ $value }} items in queue and it's growing."

  - name: containers
    interval: 30s
    rules:
      # Container high CPU
      - alert: ContainerHighCPU
        expr: sum by (name) (rate(container_cpu_usage_seconds_total{name!=""}[5m])) * 100 > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Container {{ $labels.name }} has high CPU usage"
          description: "Container CPU usage is above 80% (current: {{ $value | humanize }}%)"

      # Container high memory
      - alert: ContainerHighMemory
        expr: (container_memory_usage_bytes{name!=""} / container_spec_memory_limit_bytes{name!=""}) * 100 > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Container {{ $labels.name }} has high memory usage"
          description: "Container memory usage is above 80% of limit (current: {{ $value | humanize }}%)"

      # Container restarting
      - alert: ContainerRestarting
        expr: rate(container_last_seen{name!=""}[5m]) > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Container {{ $labels.name }} is restarting frequently"
          description: "Container has restarted {{ $value }} times in the last 5 minutes."
